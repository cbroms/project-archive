<!DOCTYPE html>
<html class="no-js" lang="">
    <head>
        <meta charset="utf-8" />
        <meta http-equiv="x-ua-compatible" content="ie=edge" />
        <title>Zoom Morsels - Archive</title>
        <meta name="author" content="Christian Broms" />
        <meta name='desription' content='Christian Broms' archived projects. A collection of websites, visualizations, photographs, and designs.'>
        <meta name="description" content="" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <!-- <link rel="stylesheet" href="https://use.typekit.net/ald2mmj.css" /> -->
        <link rel="stylesheet" href="/static/style.css" />
        <script src="/static/main.js"></script>
    </head>
    <body>
        <div id="header">
            <a
                id="header-link"
                href="/"
                title="Home"
                style="left: 10px; top: 10px; font-weight: 600;"
            ></a>
            <!-- <a href="/" style="left: 15px; top: 20px;">~CB</a> -->
        </div>
        <div id='content'><div id='date'>Apr 30, 2020</div><h1>Zoom Morsels: a series of small interactions using facetracking to control your presence on Zoom.</h1>
<p>Description from <a href="https://courses.ideate.cmu.edu/60-461/s2020/cbromsandrew-cmu-edu/04/30/christian-final/">the original post</a>:</p>
<p>I created a system that captures people’s faces from Zoom, tracks them over time, uses them to control a new environment, then sends video back to Zoom. I was most interested in finding a way to capture people’s face and gaze from their Zoom feeds without them having to make any sort of effort; just by being in the call they get to see a new view. The system runs on one computer, uses virtual cameras to extract sections of the screen for each video, then uses the facetracking library <a href="https://github.com/jjzhang166/ofxFaceTracker2">ofxFaceTracker2</a> and <a href="https://github.com/kylemcdonald/ofxCv">openCV</a> in <a href="https://openframeworks.cc/">openFrameworks</a> to re-render the video with new rules for how to interact. There’s lots of potential uses and fun games you could play with this system, once the pose data has been extracted from the video you can do nearly anything with in in openFrameworks.</p>
<iframe src="https://player.vimeo.com/video/413654044" width="700" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

<h2>The System</h2>
<p>Creating this system was mainly a process of finding a way to grab zoom video and efficiently running the facetracking model on multiple instances.  I created a virtual camera using the virtual camera plugin for OBS Studio for each person and masked out their video by hand. Then, in openFrameworks I grabbed each of the video streams and ran the facetracker on each of them on different threads. This method gets quite demanding quickly, but enables very accurate landmark detection in real time.</p>
<p><a href="/static/images/zoom-morsels/FaceTracking.jpg"><img style='background-image: url(/static/images/zoom-morsels/FaceTracking-thumb.jpg)' data-src="/static/images/zoom-morsels/FaceTracking.jpg"  alt="" src="/static/images/zoom-morsels/FaceTracking-thumb.jpg"></a></p>
<p>Here’s what happens on the computer running the capture:</p>
<ol>
<li>The section of the screen with the speaker’s video from zoom is clipped through OBS (this is done 3x when there are three people on the call)</li>
<li>OBS creates a virtual cam with this section of the screen.</li>
<li>openFrameworks uses the virtual camera stream from OBS as its input and runs a face tracking library to detect faces and draw over them.</li>
<li>A second OBS instance captures the openFrameworks window and creates a second virtual camera, which is used as the input for Zoom.</li>
</ol>
<p>The benefit of this method is that it enables very accurate facetracking, as each video stream gets its own facetracker instance. However, this gets quite demanding quickly and with more than three video streams requires much more processing power than I had available. Another limitation is that each video feed must be painstakingly cut out and turned into a virtual camera in OBS; it would be preferable if just one virtual camera instance could grab the entire grid view of participants. My <a href="https://courses.ideate.cmu.edu/60-461/s2020/cbromsandrew-cmu-edu/04/19/final-project-wip-update/">update post</a> explains more about this. I’m still working on a way to make the grid view facetracking possible.</p>
<h2>The Interactions</h2>
<p>Having developed the system, I wanted to create a few small interactions that explore some of the ways that you might be able to control Zoom, if Zoom were cooler. I took inspiration in the simplicity of some of the work done by Zach Lieberman with his <a href="https://www.instagram.com/p/Beli5JFAM0o/?taken-by=zach.lieberman">face studies</a>.</p>
<p>I was also interested in thinking about how you might interact with standard Zoom controls (like toggling video on and off) through just your face in a very basic way. For example, having the video fade away unless you move your head constantly. I created four different interaction modes and a couple of extra tidbits, including a tag game. The code that runs the facetracking and different interaction modes can be <a href="https://github.com/cbroms/zoom-zoom">found here</a>.</p><div><em>This is an archived page. Find recent projects <a href='https://christianbroms.com'>on my website</a></em></div></div>
        <div id="footer">
            <span id="copy"></span>
            <a href="/">All Archived Projects</a>
            <a href="https://christianbroms.com">Recent Projects</a>
            <a href="https://christianbroms.com/about">About and Contact</a>
            <a href="https://github.com/cbroms/project-archive"
                >This page's source code</a
            >
        </div>
    </body>
</html>
